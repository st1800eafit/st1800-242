{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo tomado de: \n",
    "# https://community.hortonworks.com/articles/84781/spark-text-analytics-uncovering-data-driven-topics.html\n",
    "# github: https://github.com/zaratsian/Spark/blob/master/text_analytics_datadriven_topics.json (con zeppelin)\n",
    "# otros ejemplos muy buenos: https://github.com/zaratsian/Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#forma 1 de crear la sesiÃ³n y el contexto Spark:\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col, expr, when, concat, lit, isnan\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actualice el path de airlines.csv a su propio directorio\n",
    "df=spark.read.csv(\"airlines.csv\", inferSchema=True, header=True)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"train_df\")\n",
    "sqlDF = spark.sql(\"SELECT * FROM train_df\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stopwords en nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "stop_words_nltk = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA, BisectingKMeans\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = spark.read.load(\"file:///opt/datasets/airlines.csv\", format=\"csv\", header=True)\n",
    "rawdata.printSchema()\n",
    "rawdata[0]\n",
    "rawdata = rawdata.fillna({'review': ''})                               # Replace nulls with blank string\n",
    "\n",
    "# Add Unique ID\n",
    "rawdata = rawdata.withColumn(\"uid\", monotonically_increasing_id())     # Create Unique ID\n",
    "\n",
    "# Generate YYYY-MM variable\n",
    "rawdata = rawdata.withColumn(\"year_month\", rawdata.date.substr(1,6))\n",
    "\n",
    "# Show rawdata (as DataFrame)\n",
    "rawdata.show(10)\n",
    "\n",
    "# Print data types\n",
    "for type in rawdata.dtypes:\n",
    "    print(type)\n",
    "\n",
    "target = rawdata.select(rawdata['rating'].cast(IntegerType()))\n",
    "target.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "#\n",
    "#   Text Pre-processing (consider using one or all of the following):\n",
    "#       - Remove common words (with stoplist)\n",
    "#       - Handle punctuation\n",
    "#       - lowcase/upcase\n",
    "#       - Stemming\n",
    "#       - Part-of-Speech Tagging (nouns, verbs, adj, etc.)\n",
    "#\n",
    "################################################################################################\n",
    "\n",
    "def cleanup_text(record):\n",
    "    text  = record[8]\n",
    "    uid   = record[9]\n",
    "    words = text.split()\n",
    "    \n",
    "    # Default list of Stopwords\n",
    "    stopwords_core = ['a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', u'are', u'arent', u'as', u'at', \n",
    "    u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by', \n",
    "    u'can', 'cant', 'come', u'could', 'couldnt', \n",
    "    u'd', u'did', u'didn', u'do', u'does', u'doesnt', u'doing', u'dont', u'down', u'during', \n",
    "    u'each', \n",
    "    u'few', 'finally', u'for', u'from', u'further', \n",
    "    u'had', u'hadnt', u'has', u'hasnt', u'have', u'havent', u'having', u'he', u'her', u'here', u'hers', u'herself', u'him', u'himself', u'his', u'how', \n",
    "    u'i', u'if', u'in', u'into', u'is', u'isnt', u'it', u'its', u'itself', \n",
    "    u'just', \n",
    "    u'll', \n",
    "    u'm', u'me', u'might', u'more', u'most', u'must', u'my', u'myself', \n",
    "    u'no', u'nor', u'not', u'now', \n",
    "    u'o', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'our', u'ours', u'ourselves', u'out', u'over', u'own', \n",
    "    u'r', u're', \n",
    "    u's', 'said', u'same', u'she', u'should', u'shouldnt', u'so', u'some', u'such', \n",
    "    u't', u'than', u'that', 'thats', u'the', u'their', u'theirs', u'them', u'themselves', u'then', u'there', u'these', u'they', u'this', u'those', u'through', u'to', u'too', \n",
    "    u'under', u'until', u'up', \n",
    "    u'very', \n",
    "    u'was', u'wasnt', u'we', u'were', u'werent', u'what', u'when', u'where', u'which', u'while', u'who', u'whom', u'why', u'will', u'with', u'wont', u'would', \n",
    "    u'y', u'you', u'your', u'yours', u'yourself', u'yourselves']\n",
    "    \n",
    "    # Custom List of Stopwords - Add your own here\n",
    "    stopwords_custom = ['']\n",
    "    stopwords = stopwords_core + stopwords_custom\n",
    "    stopwords = [word.lower() for word in stopwords]    \n",
    "    \n",
    "    text_out = [re.sub('[^a-zA-Z0-9]','',word) for word in words]                                       # Remove special characters\n",
    "    text_out = [word.lower() for word in text_out if len(word)>2 and word.lower() not in stopwords]     # Remove stopwords and words under X length\n",
    "    return text_out\n",
    "\n",
    "udf_cleantext = udf(cleanup_text , ArrayType(StringType()))\n",
    "clean_text = rawdata.withColumn(\"words\", udf_cleantext(struct([rawdata[x] for x in rawdata.columns])))\n",
    "\n",
    "#tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"words\")\n",
    "#wordsData = tokenizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency Vectorization  - Option 1 (Using hashingTF): \n",
    "#hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "#featurizedData = hashingTF.transform(clean_text)\n",
    "\n",
    "# Term Frequency Vectorization  - Option 2 (CountVectorizer)    : \n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 1000)\n",
    "cvmodel = cv.fit(clean_text)\n",
    "featurizedData = cvmodel.transform(clean_text)\n",
    "\n",
    "vocab = cvmodel.vocabulary\n",
    "vocab_broadcast = sc.broadcast(vocab)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 25 Data-Driven Topics:\n",
    "lda = LDA(k=25, seed=123, optimizer=\"em\", featuresCol=\"features\")\n",
    "\n",
    "ldamodel = lda.fit(rescaledData)\n",
    "\n",
    "#model.isDistributed()\n",
    "#model.vocabSize()\n",
    "\n",
    "ldatopics = ldamodel.describeTopics()\n",
    "#ldatopics.show(25)\n",
    "\n",
    "def map_termID_to_Word(termIndices):\n",
    "    words = []\n",
    "    for termID in termIndices:\n",
    "        words.append(vocab_broadcast.value[termID])\n",
    "    \n",
    "    return words\n",
    "\n",
    "udf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "ldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))\n",
    "ldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc).show(50,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaResults = ldamodel.transform(rescaledData)\n",
    "\n",
    "ldaResults.select('id','airline','date','cabin','rating','words','features','topicDistribution').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breakout_array(index_number, record):\n",
    "    vectorlist = record.tolist()\n",
    "    return vectorlist[index_number]\n",
    "\n",
    "udf_breakout_array = udf(breakout_array, FloatType())\n",
    "\n",
    "# Extract document weights for Topics 12 and 20\n",
    "enrichedData = ldaResults                                                                   \\\n",
    "        .withColumn(\"Topic_12\", udf_breakout_array(lit(12), ldaResults.topicDistribution))  \\\n",
    "        .withColumn(\"topic_20\", udf_breakout_array(lit(20), ldaResults.topicDistribution))            \n",
    "\n",
    "enrichedData.select('id','airline','date','cabin','rating','words','features','topicDistribution','Topic_12','Topic_20').show()\n",
    "\n",
    "#enrichedData.agg(max(\"Topic_12\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
