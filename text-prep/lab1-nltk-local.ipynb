{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# \n",
    "# Universidad EAFIT \n",
    "# 2024-2\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar las librerias necesarias\n",
    "## 1. nltk para 'procesamiento natural del lenguaje'\n",
    "## 2. pandas para procesamiento de dataframes, muy usado en preparación de datos\n",
    "## 3. re - expresiones regulares\n",
    "## 4. numpy, codecs, etc - otras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk\n",
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directorios (path) de entrada y salida:\n",
    "# \n",
    "path_in=\"../datasets/gutenberg-es/\"\n",
    "path_out=\"../out/\"\n",
    "filenametxt='pg2000.txt'\n",
    "filenamecleantxt='pg2000_clean.txt'\n",
    "filenamecsv='pg2000.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus de nltk para 'tokenizer', 'stopwords' y 'words' (diccionarios)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo de como nltk tokeniza:\n",
    "texto=\"texto libre que permite crear     hiso20091iras epor--4 no s#e preocupe \\n hola mundo cruel\"\n",
    "tokens = nltk.word_tokenize(texto)\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note la estrategia de tokenizar con sentencias simples de python, \n",
    "# ¿ cual le parece mejor?\n",
    "# y note la diferencia entre .split() y .split(' ')\n",
    "texto=\"texto libre que permite crear     hiso20091iras            epor--4 no s#e preocupe \\n hola mundo cruel\"\n",
    "tokens = texto.split()\n",
    "print(len(tokens))\n",
    "print(tokens)\n",
    "tokens = texto.split(' ')\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords en nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "stop_words_nltk = set(stopwords.words('spanish'))\n",
    "stop_words_nltk_en = set(stopwords.words('english'))\n",
    "print(len(stop_words_nltk_en))\n",
    "print(stop_words_nltk_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permite verificar en nltk si un token pertenece a diccionario de un idioma, en este caso a 'english'\n",
    "from nltk.corpus import words as voc_en\n",
    "\n",
    "x = len(voc_en.words())\n",
    "print('tamaño del diccionario en inglés del nltk: ',x)\n",
    "\n",
    "# verifica si una palabra pertenece al diccionario:\n",
    "w = \"house\"\n",
    "if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words_nltk_en):\n",
    "    print(w,\" true\")\n",
    "else:\n",
    "    print(w,\" false\")\n",
    "    \n",
    "w = \"pepito\"\n",
    "if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words_nltk_en):\n",
    "    print(w,\" true\")\n",
    "else:\n",
    "    print(w,\" false\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer un archivo de ejemplo en .txt\n",
    "input_file = open(path_in+filenametxt, \"r\", encoding='iso-8859-1')\n",
    "filedata = input_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opción 1:\n",
    "# TOKENIZAR con .split(), \n",
    "# ELIMINAR tokens de long = 1\n",
    "# ELIMINAR caracteres que no sean alfanumericos y pasar todo a minuscula\n",
    "# REMOVER stop words con nltk\n",
    "# graficar los 20 términos más frecuentes:\n",
    "\n",
    "tokens = filedata.split()\n",
    "tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "# tokens=[word for word in tokens if word.isalpha()] si en vez de re.sub(r'[^A-Za-z0-9]+','',w) hace esto, que pasa?\n",
    "tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "tokens = [w for w in tokens if w not in stop_words_nltk]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "topwords = fdist.most_common(20)\n",
    "print (topwords)\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opción 2:\n",
    "# TOKENIZAR con nltk, \n",
    "# ELIMINAR tokens de long = 1\n",
    "# ELIMINAR caracteres que no sean alfanumericos\n",
    "# REMOVER stop words\n",
    "# graficar los 20 términos más frecuentes:\n",
    "\n",
    "tokens = nltk.word_tokenize(filedata)\n",
    "tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "tokens = [w for w in tokens if w not in stop_words_nltk]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "print (topwords)\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming con NLTK\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "# probar cada una de las siguientes opciones: porter y lancaster.\n",
    "#tokens = [porter.stem(w) for w in tokens]\n",
    "tokens = [lancaster.stem(w) for w in tokens]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization con NLTK\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# probar cada una de las siguientes opciones: \n",
    "#tokens = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in tokens ]\n",
    "tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens ]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volver a leer el archivo ejemplo en .txt\n",
    "#input_file = open(path_in+filenametxt, \"r\",encoding='iso-8859-1')\n",
    "input_file = open(path_in+filenametxt, \"r\")\n",
    "output_file_clean = open(path_out+filenamecleantxt, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for line in input_file:\n",
    "    line_clean = \"\"\n",
    "    tokens = nltk.word_tokenize(line)\n",
    "    tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "    tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    tokens = [w for w in tokens if w not in stop_words_nltk]\n",
    "    #tokens = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in tokens]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "    #tokens = [porter.stem(w) for w in tokens]\n",
    "    tokens = [lancaster.stem(w) for w in tokens]\n",
    "    \n",
    "    for w in tokens:\n",
    "        line_clean=line_clean+w+\" \"\n",
    "            \n",
    "    if (line_clean!=\"\"):\n",
    "        line_clean=line_clean+\"\\n\"\n",
    "        output_file_clean.write(line_clean)\n",
    "output_file_clean.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_clean = open(path_out+filenamecleantxt, \"r\", encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedata = input_file_clean.read()\n",
    "tokens = filedata.split()\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = fdist.most_common(len(fdist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(path_out+filenamecsv, 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow([\"word\", \"frecuency\"])\n",
    "    writer.writerows(word_freq)\n",
    "\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top 30 words\n",
    "top_words = word_freq[:20]\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(top_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x,y = zip(*top_words)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame(top_words)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(df[0],df[1])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"frecuency\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
